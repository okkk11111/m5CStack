{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from collections import Counter\n",
    "import re, os, sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, roc_curve, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, recall_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import iFeatureOmegaCLI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(input_txt_path, feature_name):\n",
    "    feature = iFeatureOmegaCLI.iRNA(input_txt_path)\n",
    "\n",
    "    feature.get_descriptor(feature_name)\n",
    "\n",
    "    return feature.encodings.reset_index(drop=True)\n",
    "\n",
    "feature_names = [\n",
    "    \"CKSNAP type 1\", \"Z_curve_48bit\", \"Kmer type 1\", \"Mismatch\", \"DBE\", \n",
    "    \"ENAC\", \"NAC\", \"MMI\", \"NCP\", \"PS2\", \"ASDC\"\n",
    "]\n",
    "\n",
    "inputfile = '/train_drosophila.fasta'\n",
    "\n",
    "all_features = {}\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    feature_data = extract_feature(inputfile, feature_name)\n",
    "    all_features[feature_name] = feature_data\n",
    "\n",
    "    length = len(feature_data)\n",
    "    pos_lab = np.ones(int(length / 2))\n",
    "    neg_lab = np.zeros(int(length / 2))\n",
    "    new_column = np.concatenate((pos_lab, neg_lab), axis=0)\n",
    "    \n",
    "    feature_data['new_column'] = new_column\n",
    "    labels = feature_data['new_column'].to_numpy()\n",
    "\n",
    "    data_io = feature_data.drop(columns=['new_column']).to_numpy()\n",
    "\n",
    "    '''\n",
    "    c = list(zip(data_only, labels))\n",
    "    random.Random(100).shuffle(c)\n",
    "    data_io, labels = zip(*c)\n",
    "    '''\n",
    "\n",
    "    feature_key = feature_name.lower().replace(\" type 1\", \"\").replace(\"_48bit\", \"\").replace(\" \", \"_\").replace(\"2\",\"\")\n",
    "    globals()[f\"data_{feature_key}\"] = np.asarray(data_io)\n",
    "    globals()[f\"labels_{feature_key}\"] = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "xgb_cksnap = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_cksnap = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_cksnap = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_cksnap = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_cksnap = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_cksnap = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_cksnap = GaussianNB()  \n",
    "\n",
    "knn_cksnap = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_cksnap = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_cksnap = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_cksnap = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_cksnap = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_cksnap = [lgbm_cksnap, svm_cksnap, lg_cksnap, mlp_cksnap, knn_cksnap, ada_cksnap, et_cksnap, cb_cksnap, nb_cksnap, xgb_cksnap, rf_cksnap]\n",
    "\n",
    "######\n",
    "\n",
    "xgb_Z_curve = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_Z_curve = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_Z_curve = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_Z_curve = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_Z_curve = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_Z_curve = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_Z_curve = GaussianNB()  \n",
    "\n",
    "knn_Z_curve = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_Z_curve = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_Z_curve = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_Z_curve = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_Z_curve = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_Z_curve = [lgbm_Z_curve, svm_Z_curve, lg_Z_curve, mlp_Z_curve, knn_Z_curve, ada_Z_curve, et_Z_curve, cb_Z_curve, nb_Z_curve, xgb_Z_curve, rf_Z_curve]\n",
    "\n",
    "###\n",
    "xgb_kmer = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_kmer = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_kmer = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_kmer = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_kmer = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_kmer = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_kmer = GaussianNB()  \n",
    "\n",
    "knn_kmer = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_kmer = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_kmer = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_kmer = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_kmer = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_kmer = [lgbm_kmer, svm_kmer, lg_kmer, mlp_kmer, knn_kmer, ada_kmer, et_kmer, cb_kmer, nb_kmer, xgb_kmer, rf_kmer]\n",
    "\n",
    "####\n",
    "\n",
    "xgb_mismatch = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_mismatch = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_mismatch = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_mismatch = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_mismatch = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_mismatch = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_mismatch = GaussianNB()  \n",
    "\n",
    "knn_mismatch = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_mismatch = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_mismatch = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_mismatch = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_mismatch = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_mismatch = [lgbm_mismatch, svm_mismatch, lg_mismatch, mlp_mismatch, knn_mismatch, ada_mismatch, et_mismatch, cb_mismatch, nb_mismatch, xgb_mismatch, rf_mismatch]\n",
    "\n",
    "###\n",
    "xgb_dbe = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_dbe = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_dbe = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_dbe = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_dbe = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_dbe = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_dbe = GaussianNB()  \n",
    "\n",
    "knn_dbe = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_dbe = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_dbe = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_dbe = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_dbe = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_dbe = [lgbm_dbe, svm_dbe, lg_dbe, mlp_dbe, knn_dbe, ada_dbe, et_dbe, cb_dbe, nb_dbe, xgb_dbe, rf_dbe]\n",
    "\n",
    "###\n",
    "xgb_enac = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_enac = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_enac = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_enac = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_enac = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_enac = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_enac = GaussianNB()  \n",
    "\n",
    "knn_enac = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_enac = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_enac = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_enac = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_enac = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_enac = [lgbm_enac, svm_enac, lg_enac, mlp_enac, knn_enac, ada_enac, et_enac, cb_enac, nb_enac, xgb_enac, rf_enac]\n",
    "\n",
    "####\n",
    "\n",
    "xgb_nac = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_nac = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_nac = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_nac = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_nac = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_nac = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_nac = GaussianNB()  \n",
    "\n",
    "knn_nac = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_nac = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_nac = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_nac = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_nac = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_nac = [lgbm_nac, svm_nac, lg_nac, mlp_nac, knn_nac, ada_nac, et_nac, cb_nac, nb_nac, xgb_nac, rf_nac]\n",
    "\n",
    "###\n",
    "xgb_mmi = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_mmi = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_mmi = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_mmi = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_mmi = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_mmi = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_mmi = GaussianNB()  \n",
    "\n",
    "knn_mmi = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_mmi = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_mmi = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_mmi = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_mmi = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_mmi = [lgbm_mmi, svm_mmi, lg_mmi, mlp_mmi, knn_mmi, ada_mmi, et_mmi, cb_mmi, nb_mmi, xgb_mmi, rf_mmi]\n",
    "\n",
    "####\n",
    "\n",
    "xgb_ncp = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_ncp = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_ncp = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_ncp = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_ncp = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_ncp = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_ncp = GaussianNB()  \n",
    "\n",
    "knn_ncp = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_ncp = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_ncp = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_ncp = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_ncp = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_ncp = [lgbm_ncp, svm_ncp, lg_ncp, mlp_ncp, knn_ncp, ada_ncp, et_ncp, cb_ncp, nb_ncp, xgb_ncp, rf_ncp]\n",
    "\n",
    "###\n",
    "xgb_ps = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_ps = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_ps = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_ps = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_ps = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_ps = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_ps = GaussianNB()  \n",
    "\n",
    "knn_ps = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_ps = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_ps = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_ps = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_ps = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_ps = [lgbm_ps, svm_ps, lg_ps, mlp_ps, knn_ps, ada_ps, et_ps, cb_ps, nb_ps, xgb_ps, rf_ps]\n",
    "\n",
    "####\n",
    "\n",
    "xgb_asdc = XGBClassifier(learning_rate=0.017965145536083, max_depth=11, \n",
    "                min_child_weight=1, gamma=0.17426232112256684, \n",
    "                colsample_bytree=0.1916286359142644, n_estimators=625, seed=14)\n",
    "\n",
    "lgbm_asdc = lgb.LGBMClassifier(learning_rate=0.011038126658226765, max_depth=12,\n",
    "                min_child_samples=5, min_child_weight=0.4318851550452938, min_split_gain=0.025127530387291663,\n",
    "                n_estimators=471, num_leaves=31) \n",
    "\n",
    "svm_asdc = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "rf_asdc = RandomForestClassifier(max_depth=80, bootstrap=False, max_features = 'sqrt',\n",
    "                                    n_estimators=400, min_samples_split=3,random_state=0)\n",
    "lg_asdc = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='lbfgs', random_state=14)\n",
    "\n",
    "mlp_asdc = MLPClassifier(hidden_layer_sizes=(210,), activation='logistic', solver='adam', random_state=10)\n",
    "\n",
    "nb_asdc = GaussianNB()  \n",
    "\n",
    "knn_asdc = KNeighborsClassifier(n_neighbors=7, weights='distance', p=2)  \n",
    "\n",
    "ada_asdc = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100, learning_rate=0.5, random_state=0\n",
    ")  \n",
    "\n",
    "dt_asdc = DecisionTreeClassifier(\n",
    "    max_depth=20, min_samples_split=4, min_samples_leaf=2, random_state=0\n",
    ")  \n",
    "\n",
    "et_asdc = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=4, min_samples_leaf=2,\n",
    "    max_features='sqrt', bootstrap=True, random_state=0\n",
    ")  \n",
    "cb_asdc = CatBoostClassifier(learning_rate=0.25659939142290733, max_depth=4,iterations=90,\n",
    "                                  boosting_type='Plain')\n",
    "    \n",
    "classifiers_asdc = [lgbm_asdc, svm_asdc, lg_asdc, mlp_asdc, knn_asdc, ada_asdc, et_asdc, cb_asdc, nb_asdc, xgb_asdc, rf_asdc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6372a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(classifier, X_train, X_test, y_train, y_test, model_type, fold):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    y_prob = classifier.predict_proba(X_test)[:, 1] if hasattr(classifier, \"predict_proba\") else None\n",
    "\n",
    "    #model_path = f'/human_{classifier.__class__.__name__}_{model_type}_fold{fold}.pickle'\n",
    "    #with open(model_path, 'wb') as model_file:\n",
    "     #   pickle.dump(classifier, model_file)\n",
    "    \n",
    "    return y_pred, y_prob, acc\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=4)\n",
    "\n",
    "datasets = {\n",
    "    'cksnap': (data_cksnap, labels_cksnap, classifiers_cksnap),\n",
    "    'Z_curve': (data_z_curve, labels_z_curve, classifiers_Z_curve),\n",
    "    'kmer': (data_kmer, labels_kmer, classifiers_kmer),\n",
    "    'mismatch': (data_mismatch, labels_mismatch, classifiers_mismatch),\n",
    "    'dbe': (data_dbe, labels_dbe, classifiers_dbe),\n",
    "    'enac': (data_enac, labels_enac, classifiers_enac),\n",
    "    'nac': (data_nac, labels_nac, classifiers_nac),\n",
    "    'mmi': (data_mmi, labels_mmi, classifiers_mmi),\n",
    "    'ncp': (data_ncp, labels_ncp, classifiers_ncp),\n",
    "    'ps': (data_ps, labels_ps, classifiers_ps),\n",
    "    'asdc': (data_asdc, labels_asdc, classifiers_asdc),\n",
    "}\n",
    "\n",
    "all_y_tests = []\n",
    "\n",
    "all_preds_list = []\n",
    "all_probs_list = []\n",
    "all_preds_dict = {}\n",
    "all_probs_dict = {}\n",
    "\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(datasets['cksnap'][0],datasets['cksnap'][1])):\n",
    "    fold_preds = []  \n",
    "    fold_probs = []\n",
    "    \n",
    "    for model_type, (data, labels, classifiers) in datasets.items():\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            classifier_name = classifier.__class__.__name__\n",
    "\n",
    "            y_pred, y_prob, _ = train_and_save_model(classifier, X_train, X_test, y_train, y_test, model_type, fold)\n",
    "            \n",
    "            if (model_type, classifier_name) not in all_preds_dict:\n",
    "                all_preds_dict[(model_type, classifier_name)] = []\n",
    "                all_probs_dict[(model_type, classifier_name)] = []\n",
    "            \n",
    "\n",
    "            all_preds_dict[(model_type, classifier_name)].append(y_pred)\n",
    "            if y_prob is not None:\n",
    "                all_probs_dict[(model_type, classifier_name)].append(y_prob)\n",
    "            \n",
    "            print(f'{classifier_name} Accuracy {model_type} fold {fold}: {accuracy_score(y_test, y_pred)}')\n",
    "    all_y_tests.append(y_test)\n",
    "\n",
    "for (model_type, classifier_name), preds_list in all_preds_dict.items():\n",
    "    concatenated_preds = np.concatenate(preds_list, axis=0)\n",
    "    all_preds_list.append(concatenated_preds)\n",
    "\n",
    "for (model_type, classifier_name), probs_list in all_probs_dict.items():\n",
    "    if probs_list:\n",
    "        concatenated_probs = np.concatenate(probs_list, axis=0)\n",
    "        all_probs_list.append(concatenated_probs)\n",
    "\n",
    "all_y_testss = np.concatenate(all_y_tests, axis=0)            \n",
    "all_y_preds = np.column_stack(all_preds_list)\n",
    "all_y_probs = np.column_stack(all_probs_list) if all_probs_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#meta_classifier = LogisticRegression(random_state=0)\n",
    "meta_classifier = RandomForestClassifier(max_depth=80, bootstrap=True, max_features = 'sqrt',\n",
    "                                    n_estimators=600, min_samples_split=4,random_state=0)\n",
    "#meta_classifier = SVC(C=2, kernel='rbf', gamma='scale')\n",
    "\n",
    "acc_ens_list = []\n",
    "mcc_ens_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "f1_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "auc_list = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(all_y_preds, all_y_testss)):\n",
    "    X_train_ens, X_test_ens = all_y_preds[train_index], all_y_preds[test_index]\n",
    "    y_train_ens, y_test_ens = all_y_testss[train_index], all_y_testss[test_index]\n",
    "\n",
    "    meta_classifier.fit(X_train_ens, y_train_ens)\n",
    "    #meta_model_path = f'/human_META_fold{fold}.pickle'\n",
    "    #with open(meta_model_path, 'wb') as model_file:\n",
    "     #   pickle.dump(meta_classifier, model_file)\n",
    "    \n",
    "    y_pred_ens = meta_classifier.predict(X_test_ens)\n",
    "\n",
    "    acc_ens = accuracy_score(y_test_ens, y_pred_ens)\n",
    "    mcc_ens = matthews_corrcoef(y_test_ens, y_pred_ens)\n",
    "\n",
    "    confusion = confusion_matrix(y_test_ens, y_pred_ens)\n",
    "    TN, FP, FN, TP = confusion.ravel()\n",
    "    sensitivity = TP / float(TP + FN)\n",
    "    specificity = TN / float(TN + FP)\n",
    "    F1Score = (2 * TP) / float(2 * TP + FP + FN)\n",
    "\n",
    "    y_pred_prob = meta_classifier.predict_proba(X_test_ens)[:, 1]  \n",
    "    ROCArea = roc_auc_score(y_test_ens, y_pred_prob)\n",
    "\n",
    "    acc_ens_list.append(acc_ens)\n",
    "    mcc_ens_list.append(mcc_ens)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "    f1_list.append(F1Score)\n",
    "    auc_list.append(ROCArea)\n",
    "    \n",
    "    print(f'Fold {fold + 1} - Acc: {acc_ens}, MCC: {mcc_ens}, Sensitivity: {sensitivity}, Specificity: {specificity}, AUC: {ROCArea}, F1: {F1Score}')\n",
    "\n",
    "print('Mean Acc Ensemble: ', np.mean(acc_ens_list))\n",
    "print('Mean Sens Ensemble: ', np.mean(sensitivity_list))\n",
    "print('Mean Spe Ensemble: ', np.mean(specificity_list))\n",
    "print('Mean MCC Ensemble: ', np.mean(mcc_ens_list))\n",
    "print('Mean AUC Ensemble: ', np.mean(auc_list))\n",
    "print('Mean F1 Ensemble: ', np.mean(f1_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
